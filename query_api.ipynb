{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai==0.28 tqdm gdown"
      ],
      "metadata": {
        "id": "Qdpe9PfaZ6yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import openai\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from google.colab import auth\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "VJrqnDoDZ9ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlkUJ-06M-R9"
      },
      "source": [
        "## query_openai_api.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chat_prompts import chat_prompts\n",
        "\n",
        "def ask_gpt_chat(model_name, sys_prompt, user_prompt, temperature=0, top_p=1):\n",
        "    MAX_API_RETRY = 5\n",
        "    for i in range(MAX_API_RETRY):\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                ],\n",
        "                temperature=temperature,\n",
        "                top_p=top_p\n",
        "            )\n",
        "            return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}. Retry {i + 1}/{MAX_API_RETRY}\")\n",
        "            time.sleep(5 * (i + 1))\n",
        "    return None"
      ],
      "metadata": {
        "id": "cH44ODQ5JNWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_question_text(prompt_type, question):\n",
        "    return 'I ' + question['text_first_person'].lower()"
      ],
      "metadata": {
        "id": "kCsInIeCJNZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kzADv44NACh"
      },
      "outputs": [],
      "source": [
        "def read_ocean(filename, ipip):\n",
        "    ocean_data = []\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        next(csvreader)\n",
        "        for row in csvreader:\n",
        "            if ipip == '120':\n",
        "                text_first_person, text_second_person, label, key = row[4], row[5], row[7], row[9]\n",
        "            elif ipip == '300':\n",
        "                text_first_person, text_second_person, label, key = row[4], row[5], row[6], row[8]\n",
        "            ocean_data.append({\n",
        "                'text_first_person': text_first_person,\n",
        "                'text_second_person': text_second_person,\n",
        "                'label': label,\n",
        "                'key': key\n",
        "            })\n",
        "    return ocean_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    ipip_dataset = '300'\n",
        "    filename = '/Tests/ocean_' + ipip_dataset + '_corrected.csv'\n",
        "    ocean_data = read_ocean(filename, ipip_dataset)\n",
        "\n",
        "    model_name = 'gpt-3.5-turbo'\n",
        "    temperature = 0.01\n",
        "    top_p = 1\n",
        "    max_tokens = 120\n",
        "\n",
        "    output_directory = '/Data/Outputs_' + ipip_dataset + '/' + model_name.replace('/', '-') + '/'\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    #START EXPERIMENTS\n",
        "    for prompt_type in chat_prompts.keys():\n",
        "        output_filename = output_directory + prompt_type + '.json'\n",
        "\n",
        "        system_message = chat_prompts[prompt_type]['system_message']\n",
        "        user_message = chat_prompts[prompt_type]['user_message']\n",
        "\n",
        "        output_dict = {\n",
        "            'system_prompt': system_message,\n",
        "            'user_prompt': user_message,\n",
        "            'responses': []\n",
        "        }\n",
        "\n",
        "        for q, question in enumerate(ocean_data):\n",
        "            print(prompt_type, q)\n",
        "\n",
        "            question_text = get_question_text(prompt_type, question)\n",
        "            system_prompt = system_message.replace('{item}', question_text)\n",
        "            user_prompt = user_message.replace('{item}', question_text)\n",
        "\n",
        "            response = None\n",
        "            while not response:\n",
        "                response = ask_gpt_chat(model_name, system_prompt, user_prompt)\n",
        "\n",
        "            #store question and response\n",
        "            question['input_prompt_system'] = system_prompt\n",
        "            question['input_prompt_user'] = user_prompt\n",
        "            question['processed_response'] = response\n",
        "\n",
        "            output_dict['responses'].append(question)\n",
        "\n",
        "            json_data = json.dumps(output_dict)\n",
        "            with open(output_filename, \"w\") as file:\n",
        "                file.write(json_data)\n",
        "\n",
        "            with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(output_dict, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "S2h9vZqFLLJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#query_llama_api"
      ],
      "metadata": {
        "id": "2ZnYXWrCNY02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_openrouter(model_name, input_prompt, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = None\n",
        "            response = requests.post(\n",
        "            url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "                  data=json.dumps({\n",
        "            \"model\": \"meta-llama/llama-2-70b-chat\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": input_prompt}],\n",
        "\n",
        "                })\n",
        "              )\n",
        "            if response.status_code == 200:\n",
        "                content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "                return content\n",
        "            else:\n",
        "                print(f\"Attempt {attempt + 1} failed with status code {response.status_code}\")\n",
        "                time.sleep(2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed with error: {str(e)}\")\n",
        "            time.sleep(2)\n",
        "\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "HLFBhp8bSeJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    ipip_dataset = '300'\n",
        "    filename = '/Tests/ocean_' + ipip_dataset + '_corrected.csv'\n",
        "    ocean_data = read_ocean(filename, ipip_dataset)\n",
        "\n",
        "    model_name = 'meta-llama/llama-2-70b-chat'\n",
        "    temperature = 0.01\n",
        "    top_p = 1\n",
        "    max_tokens = 120\n",
        "\n",
        "    output_directory = '/Data/Outputs_' + ipip_dataset + '/' + model_name.replace('/', '-') + '/'\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    #START EXPERIMENTS\n",
        "    for prompt_type in chat_prompts.keys():\n",
        "        output_filename = output_directory + prompt_type + '.json'\n",
        "\n",
        "        system_message = chat_prompts[prompt_type]['system_message']\n",
        "        user_message = chat_prompts[prompt_type]['user_message']\n",
        "\n",
        "        output_dict = {}\n",
        "        output_dict['system_prompt'] = system_message\n",
        "        output_dict['user_prompt'] = user_message\n",
        "        output_dict['responses'] = []\n",
        "\n",
        "                ## Llama prompt type 2 - original\n",
        "        input_message = \"<s>[INST] <<SYS>>\\n\"\n",
        "        input_message += system_message.strip() + \"\\n\"\n",
        "        input_message += \"<</SYS>>\\n\\n\"\n",
        "        input_message += user_message.strip() + \" [/INST]\"\n",
        "\n",
        "        for q, question in enumerate(ocean_data):\n",
        "            start_time = time.time()\n",
        "            question_text = get_question_text(prompt_type, question)\n",
        "            input_prompt = input_message.replace('{item}', question_text)\n",
        "\n",
        "            input_dict = {\n",
        "                \"prompt\": input_prompt,\n",
        "                \"temperature\": temperature,\n",
        "                \"top_p\": top_p,\n",
        "                \"max_tokens\": max_tokens\n",
        "            }\n",
        "\n",
        "            #store question and response\n",
        "            response = ask_openrouter(model_name, input_prompt)\n",
        "            question['input_prompt'] = input_prompt\n",
        "            question['response'] = response\n",
        "            question['processed_response'] = response.replace(input_prompt, '')\n",
        "            print(response)\n",
        "            output_dict['responses'].append(question)\n",
        "\n",
        "            json_data = json.dumps(output_dict)\n",
        "            with open(output_filename, \"w\") as file:\n",
        "                file.write(json_data)"
      ],
      "metadata": {
        "id": "hhc03aWJOwCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}